# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10lUfjeNUeHLoAzz093ZTPwrTKBGHgqFg
"""

from google.colab import files
uploaded = files.upload()

import io
import pandas as pd
import numpy as np

data = pd.read_csv(io.BytesIO(uploaded['cses4_cut.csv']))

voted = data['voted'] # Seperating the result and the parameters
del data['voted']
del data['Unnamed: 0']

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

test = SelectKBest(score_func=chi2, k='all')                # Checking the meaningful parameters that have huge affect on the result
fit = test.fit(data, voted)
feat_Score = fit.scores_

d = {'Feature': data.columns, 'Feature_Score': feat_Score}  # Storing the scores of the features and sorting them to see their importance easily.
df = pd.DataFrame(d)
df.sort_values('Feature_Score', ascending=False)[:15]       # Here we see first 12 features make more sense

reconstructedData = data[['D2026','D2027','D2029','D2021','D2011','D2030','D2022','D2028','D2023','D2015','age','D2016']] # Reconstructing the parametes considering their importance
reconstructedData = reconstructedData.sort_index(axis=1)

from sklearn.model_selection import train_test_split # Spliting the data to train and test the model
Xtrain, Xtest, ytrain, ytest = train_test_split(reconstructedData, voted, test_size=0.2, random_state=1)

from sklearn.linear_model import LogisticRegression

log_R = LogisticRegression()
log_R.fit(Xtrain, ytrain)
log_R_y_head = log_R.predict(Xtest) # Predicting the Y results with the trained model using the test data.

log_Accuracy = accuracy_score(ytest, log_R_y_head) # Compairing the performance of the model by using the predicted values and actual values.
cv_log = cross_val_score(log_R, Xtrain, ytrain, cv=8) # Besides to test and train split, cross validating the model with the train data
cv_log_mean = cv_log.mean() # mean accuracy of the folded model
cv_log_std = cv_log.std() # std of the folded model

from sklearn.naive_bayes import GaussianNB

gaus_NB= GaussianNB()
gaus_NB.fit(Xtrain, ytrain)
gaus_NB_y_head=gaus_NB.predict(Xtest)

gaus_NB_Accuracy = accuracy_score(ytest, gaus_NB_y_head)
cv_gausNB = cross_val_score(gaus_NB, Xtrain, ytrain, cv=8)
cv_gausNB_mean = cv_gausNB.mean()
cv_gausNB_std = cv_gausNB.std()

from sklearn.ensemble import RandomForestClassifier

randFC = RandomForestClassifier()
randFC.fit(Xtrain, ytrain)
radnFC_y_head=randFC.predict(Xtest)

randFC_Accuracy=accuracy_score(ytest,radnFC_y_head)
cv_randFC = cross_val_score(randFC, Xtrain, ytrain, cv=8)
cv_randFC_mean = cv_randFC.mean()
cv_randFC_std = cv_randFC.std()

from sklearn.neighbors import KNeighborsClassifier

n=np.arange(1,26)
Kth_N_Accuracy=np.zeros(len(n))   # Since number of the neighbors has impact on the performance first deciding the optimum number of neighbors.

for i in range(len(n)):
    KNN_C = KNeighborsClassifier(i+1)
    KNN_C.fit(Xtrain, ytrain)
    KNN_C_y_head = KNN_C.predict(Xtest)

    cv_KNN = cross_val_score(KNN_C, Xtrain, ytrain, cv=8)
    cv_KNN_mean = cv_KNN.mean()
    Kth_N_Accuracy[i] = cv_KNN_mean

opt_N=Kth_N_Accuracy.argmax()+1 # Choosing the number of neighbors considering the accuracies.


KNN_C = KNeighborsClassifier(opt_N)
KNN_C.fit(Xtrain, ytrain)
KNN_C_y_head = KNN_C.predict(Xtest)

KNN_C_Accuracy = accuracy_score(ytest, KNN_C_y_head)
cv_KNN = cross_val_score(KNN_C, Xtrain, ytrain, cv=8)
cv_KNN_mean = cv_KNN.mean()
cv_KNN_std = cv_KNN.std()

from sklearn.svm import LinearSVC

L_SVC = LinearSVC()
L_SVC.fit(Xtrain, ytrain)
L_SVC_y_head = L_SVC.predict(Xtest)

L_SVC_Accuracy = accuracy_score(ytest, L_SVC_y_head)
cv_L_SVC = cross_val_score(L_SVC, Xtrain, ytrain, cv=8)
cv_L_SVC_mean = cv_L_SVC.mean()
cv_L_SVC_std = cv_L_SVC.std()

method_scores = {'Method':['Logistic Regression', 'Gaussian Naive Bayes', 'Random Forest', 'KNN', 'SVM'],
               'y_head Accuracy':[log_Accuracy, gaus_NB_Accuracy, randFC_Accuracy, KNN_C_Accuracy, L_SVC_Accuracy],
               'cv_mean':[cv_log_mean, cv_gausNB_mean, cv_randFC_mean, cv_KNN_mean, cv_L_SVC_mean],
               'cv_std':[cv_log_std, cv_gausNB_std, cv_randFC_std, cv_KNN_std, cv_L_SVC_std]
                         }
valuation = pd.DataFrame(method_scores)
valuation = valuation.sort_values(by='cv_mean', ascending=False)
valuation